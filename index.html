
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>橙橙子の小站</title>
    <meta name="author" content="Knight Cauchy" />
    <meta name="description" content="『普通』和『理所当然』究竟是什么呢？" />
    <meta name="keywords" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link rel="icon" href="/images/avatar1.jpg" />
    <link rel="preconnect" href="https://cdn.staticfile.org" />
<script src="https://cdn.staticfile.org/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.loli.net" />
<link rel="preconnect" href="https://gstatic.loli.net" crossorigin />
<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" />
<script> const mixins = {}; </script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=default"></script>


<script src="https://cdn.staticfile.org/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.org/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>





<script src="/js/lib/home.js"></script>

<link rel="stylesheet" href="/css/main.css" />

    <!-- 引用 anime.js -->
    <script src="https://s4.zstatic.net/ajax/libs/animejs/3.2.1/anime.min.js"></script>
    <!-- 添加自定义CSS文件 -->
    <!-- <link rel="stylesheet" href="/css/your_custom_file.css" /> -->
<meta name="generator" content="Hexo 6.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>橙橙子の小站</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;橙橙子の小站</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div id="home-head">
    <div id="home-background" ref="homeBackground" data-images="/images/background.jpg,/images/background1.jpeg,/images/background2.jpeg"></div>
    <div id="home-info" @click="homeClick">
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="info">
            <div class="wrap">
                <h1>橙橙子の小站</h1>
                <h3>施工中...</h3>
                <h5>『普通』和『理所当然』究竟是什么呢？</h5>
            </div>
        </span>
    </div>
</div>
<div id="home-posts-wrap" true ref="homePostsWrap">
    <div id="home-posts">
        

<div class="post">
    <a href="/2025/04/26/2025-04-26-Sat/">
        <h2 class="post-title">2025-04-26-Sat</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/26
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>为什么总在摆栏呢？</p>
            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/diary/" style="color: #03a9f4">diary</a>
        </span>
        
    </div>
    <a href="/2025/04/26/2025-04-26-Sat/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/04/25/2025-04-25-Fri/">
        <h2 class="post-title">2025-04-25-Fri</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/25
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>距离oral还有32天，明天一定要过去努力</p>
            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/diary/" style="color: #ff7d73">diary</a>
        </span>
        
    </div>
    <a href="/2025/04/25/2025-04-25-Fri/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/04/24/2025-04-24-Thu/">
        <h2 class="post-title">2025-04-24-Thu</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/24
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>距离5.27 oral还有33天！加油！</p>
            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/diary/" style="color: #ffa2c4">diary</a>
        </span>
        
    </div>
    <a href="/2025/04/24/2025-04-24-Thu/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/04/23/2025-04-23-Wed/">
        <h2 class="post-title">2025-04-23-Wed</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/23
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>距离5.27 oral还有34天！加油！</p>
            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/diary/" style="color: #03a9f4">diary</a>
        </span>
        
    </div>
    <a href="/2025/04/23/2025-04-23-Wed/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/01/02/2025-1-2/">
        <h2 class="post-title">2025/1/2</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/1/2
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            
            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2025/01/02/2025-1-2/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/07/23/HqMira/">
        <h2 class="post-title">HqMira</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/23
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>这是一个医学图像深度学习处理库，</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/07/23/HqMira/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/07/08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/">
        <h2 class="post-title">医学图像超分辨率</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/8
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="超分辨率的任务和概念"><a href="#超分辨率的任务和概念" class="headerlink" title="超分辨率的任务和概念"></a>超分辨率的任务和概念</h2><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><p><strong>PET成像原理</strong>：患者注入显影剂之后，显影剂聚集在特定区域，衰变产生正电子，正电子在正常组织中飞行一段时间后慢化，速度匹配之后才能与正常组织中电子结合湮灭（该飞行距离称为正电子射程），产生一对511keV的γ光子。由于正负电子有初始动量，这对电子并不是理想地沿完全相反的方向飞行（非共线性），存在1°以内的误差，探测器半径越大，飞行距离越远，导致的分辨率误差越大。探测器两块晶体在时间窗口内接收到光子，即认为在连线上发生了湮灭，但是同一晶体的不同位置的接收被视为同一位置，晶体的尺寸制约了最大分辨率。虽然对于511keV的光子，路径上的衰减不明显，但是由于符合探测需要确保光子能量接近，飞行路径上两个光子的衰减如果不同会直接导致该事件不被接受，造成符合事件减少，信噪比降低。重建过程中，插值和滤波算法都有可能导致分辨率降低。</p>
<h3 id="任务内容"><a href="#任务内容" class="headerlink" title="任务内容"></a>任务内容</h3><p>医学图像超分辨率（SR）任务，就是获得分辨率更高的医学图像，提供更加锐利的边界。对于PET图像而言，分辨率不高的表现就是成像系统对理想的点源和线源的响应是两个扩散函数PSF和LSF，其主要的成因包括：</p>
<ul>
<li>探测器的闪烁晶体和感光元件的尺寸不是无限小的，存在部分容积效应</li>
<li>物理效应，包括正电子射程和光子非共线性</li>
<li>重建算法的插值和滤波，导致采集信号的频率降低</li>
<li>电子系统固有噪声<br>根据对以上的成因的分析，能够提供一些先验知识，给超分辨带来一定的思路。</li>
</ul>
<h3 id="医学图像SR与自然图像SR的区别和联系"><a href="#医学图像SR与自然图像SR的区别和联系" class="headerlink" title="医学图像SR与自然图像SR的区别和联系"></a>医学图像SR与自然图像SR的区别和联系</h3><p>分辨率低，对比度弱，视觉噪声散射，组织复杂模糊。PET成像的模糊核不同于自然图像，和大量解剖和先验知识相关，与空间位置信息相关，因而下面许多DL方法会将位置信息和解剖信息作为输入。<br>对纹理和伪影的容忍度低，对像素强度、对比度更加敏感，不能接受强度方面的误判，否则容易导致误诊。</p>
<h3 id="读论文的收获"><a href="#读论文的收获" class="headerlink" title="读论文的收获"></a>读论文的收获</h3><h4 id="Image-Resolution-Improvement-Based-on-Sinogram-Super-Resolution-in-PET-2010"><a href="#Image-Resolution-Improvement-Based-on-Sinogram-Super-Resolution-in-PET-2010" class="headerlink" title="Image Resolution Improvement Based on Sinogram Super-Resolution in PET 2010"></a>Image Resolution Improvement Based on Sinogram Super-Resolution in PET 2010</h4><p>wobbling motion, penalized expectation maximization algorithm(添加了正则化项TV的期望最大算法)，将LR的正弦向量视为服从泊松分布的随机变量，均值就是利用了亚像素偏移，在原来迭代算法的基础上进行。迭代公式：<br><img src="/2024/07/08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-1.png" alt="alt text"><br>缺点：该运动是人为的，已知的，离散的，而不是接近实际情况的病人运动，而且只支持平动等极为线性和简单的运动。只做了仿真而没有别的实验，超分效果也十分有限，读这篇论文的主要收获是了解了什么是迭代重建算法，以及TV正则化方法。</p>
<h4 id="Enhancing-the-image-quality-via-transferred-deep-residual-learning-of-coarse-PET-sinograms-2018"><a href="#Enhancing-the-image-quality-via-transferred-deep-residual-learning-of-coarse-PET-sinograms-2018" class="headerlink" title="Enhancing the image quality via transferred deep residual learning of coarse PET sinograms 2018"></a>Enhancing the image quality via transferred deep residual learning of coarse PET sinograms 2018</h4><p>sinogram, deep residual learning, transfer learning，用了CNN架构，但是不是重建后处理，而是设计了专用于重建前的正弦曲线数据的网络架构，学习从LRS到HRS的映射，添加了迁移学习模块，使用外部数据作为先验知识。<br>正弦曲线LRS，插值正弦BIS，SR正弦SRS，HR正弦HRS。重建之后对应的图像是LRI IRI SRI HRI<br>还比较了在投影域和图像域应用网络的性能差异。超分辨块结构：<img src="/2024/07/08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-2.png" alt="alt text"><br>还考虑了噪声，在干净数据上进行预训练之后在lowdose的有噪声数据上继续训练。<br>实验做的模型仿真，还做了小鼠和脑部。</p>
<h4 id="Self-Supervised-Super-Resolution-PET-Using-A-Generative-Adversarial-Network-2019"><a href="#Self-Supervised-Super-Resolution-PET-Using-A-Generative-Adversarial-Network-2019" class="headerlink" title="Self Supervised Super-Resolution PET Using A Generative Adversarial Network 2019"></a>Self Supervised Super-Resolution PET Using A Generative Adversarial Network 2019</h4><p>引入HR MR anatomical image, spacial information<br>SRCNN，VDSR等模型需要监督信息，需要配对的HRLR图像进行训练，该模型排除了成对数据的需求<br>method：双gan联合训练，G1从LR到HR，输入是patch的空间信息、VDSR的输出、LRPET图像、HR MR图像，输出SR。D1比较SR和未配对的HR相似度。G2从HR到LR，不过是配对的，对HR使用SVPSF得到LR。</p>
<p>缺点：需要预训练的有监督CNN作为辅助信息，即一个使用模拟数据训练的VDSR。有的地方感觉很不可靠，例如HRMR需不需要配准？G2的HR和LR’是配对的，LR’是通过HR生成的，那D2比较的就是LR’和与之并不配对的LR。D1比较的也是与之不配对的HR，不知道如何进行。效果甚至不如VDSR，对数据的要求也不能算低，因为需要配对的HRMR。<br><img src="/2024/07/08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-3.png" alt="alt text"></p>
<h4 id="Joint-Sparse-Coding-Based-Super-Resolution-PET-Image-Reconstruction-2020"><a href="#Joint-Sparse-Coding-Based-Super-Resolution-PET-Image-Reconstruction-2020" class="headerlink" title="Joint Sparse Coding-Based Super-Resolution PET Image Reconstruction 2020"></a>Joint Sparse Coding-Based Super-Resolution PET Image Reconstruction 2020</h4><p>JSC， “coupled feature spaces” ，<br>假设了解剖图和pet在配对的特征空间中，存在一个映射能够使他们互相转换，这样他们有共同的稀疏表示。在经典的惩罚去卷积迭代重建中引入新的正则化项，由解剖图像和HR PET数据训练联合字典，具体字典学习方法还看不懂。<br>只进行了仿真实验，效果比单纯的SC好。</p>
<h4 id="Super-Resolution-PET-Imaging-Using-Convolutional-Neural-Networks-2020"><a href="#Super-Resolution-PET-Imaging-Using-Convolutional-Neural-Networks-2020" class="headerlink" title="Super-Resolution PET Imaging Using Convolutional Neural Networks 2020"></a>Super-Resolution PET Imaging Using Convolutional Neural Networks 2020</h4><p>patches multimodality imaging, partial volume correction<br>后处理模型，建立LR到HR的映射，CNN学习全局参数 比较浅层和深层CNN以及不同组合下的性能。同样引入空间信息作为输入，在各种指标上效果超过了传统方法。<br>进行了模拟现实和临床实验<br>缺点是有监督，需要配对图像，且监督信息是通过HR设备的不完美采样得到的。</p>
<h4 id="Super-resolution-in-brain-positron-emission-tomography-using-a-real-time-motion-capture-system-2023"><a href="#Super-resolution-in-brain-positron-emission-tomography-using-a-real-time-motion-capture-system-2023" class="headerlink" title="Super-resolution in brain positron emission tomography using a real-time motion capture system 2023"></a>Super-resolution in brain positron emission tomography using a real-time motion capture system 2023</h4><p>real time motion capture<br>对2010年那篇论文的补充，使用的是外部的红外摄像头进行高精度连续性地运动捕捉。稳健时空校准，列表模式有序子集期望最大化重建算法。以逐个事件为基础，矫正运动对测量反应线LOR的影响。<br>在灵长类动物上进行了实验，得到了不错的结果，具体的数学处理看不懂。<br>无需监督，属于重建算法，用运动中的多个LR生成SR</p>
<h4 id="Quasi-supervised-Learning-for-Super-resolution-PET"><a href="#Quasi-supervised-Learning-for-Super-resolution-PET" class="headerlink" title="Quasi-supervised Learning for Super-resolution PET"></a>Quasi-supervised Learning for Super-resolution PET</h4><p>cycleGan的延伸 数据patch匹配<br>低监督，假设认为大量的LR和HR之间尽管未配对，但是存在着某种共性，可以通过各种公式计算相似度，进行人为的匹配，包括病人匹配，层面匹配，patch匹配。匹配系数高的pair权重高。网络结构仍然是有监督训练的设计结构。<br><img src="/2024/07/08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-4.png" alt="alt text"><br>损失函数包括对抗损失，循环一致性和反身一致性。通过对输入数据的改变和损失函数超参数的修改，该模型可退化为有监督或无监督学习，算是综合性较好了。让人感到困惑的是简单的相似度匹配（PCC和RBF）就可以达到有监督的效果吗？也没有进行配准。<br>使用脑部pet进行了验证，指标上的效果不错。</p>
<h3 id="传统重建超分辨方法"><a href="#传统重建超分辨方法" class="headerlink" title="传统重建超分辨方法"></a>传统重建超分辨方法</h3><ul>
<li>迭代重建，没有看到解析法的论文</li>
<li>摆动运动，通过多个亚体素运动下的LR，获得高频信息得到HR</li>
<li>稀疏编码，字典学习，需要迭代优化，使用L1范数确保稀疏性，联合稀疏编码，得到共享的稀疏系数矩阵，把MRI和少量HRPET所含的解剖信息引入，认为他们位于耦合的特征空间</li>
</ul>
<h3 id="基于DL的方法"><a href="#基于DL的方法" class="headerlink" title="基于DL的方法"></a>基于DL的方法</h3><p>从自然图像迁移的VDSR，进一步的有SRCNN(浅层和深层) SSSR(双gan) CNN cycleGAN<br>有监督，直接建立从LR到HR的端到端的映射或者插值后建立残差的映射。<br>无监督，有的用gan，添加一些解剖信息作为辅助，patch的空间信息，高分辨的MR</p>
<h3 id="SR所需的数据获取和预处理模式"><a href="#SR所需的数据获取和预处理模式" class="headerlink" title="SR所需的数据获取和预处理模式"></a>SR所需的数据获取和预处理模式</h3><p>有的直接仿真，这里我还不是很清楚仿真的流程和具体的做法。但是很明确的就是这样可以预设一个非常精确的HR，因为就是我们做的膜具的形状，主要是看传统重建的效果的。传统重建SR的优势就在于此，一旦方法确定，重建即所得，而DL方法训练的后处理模型大多不可迁移，换一个其他的形状可能就无法实现SR。<br>DL方法有监督情况下的配对数据，用上下采样得到HR或者LR，然后拿去训练。</p>
<h3 id="SR的损失函数和正则化方法"><a href="#SR的损失函数和正则化方法" class="headerlink" title="SR的损失函数和正则化方法"></a>SR的损失函数和正则化方法</h3><ul>
<li>损失函数多为图像指标或者单纯的正则化</li>
<li>L1和L2正则化，前者保稀疏性，后者保光滑性。</li>
<li>TV总变差</li>
<li>稀疏字典编码（引入解剖信息）</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/07/08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/">
        <h2 class="post-title">自然图像超分辨率</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/8
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p><strong>以下都是读论文的总结</strong></p>
<h1 id="综述一篇，A-Systematic-Survey-of-Deep-Learning-based-Single-Image-Super-Resolution"><a href="#综述一篇，A-Systematic-Survey-of-Deep-Learning-based-Single-Image-Super-Resolution" class="headerlink" title="综述一篇，A Systematic Survey of Deep Learning-based Single-Image Super-Resolution"></a>综述一篇，A Systematic Survey of Deep Learning-based Single-Image Super-Resolution</h1><p>下图是论文的架构，后面的论文都是该综述中引用到的，觉得有用就可以去读。<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image.png" alt="alt text"></p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>高分辨图像Y，在H*R的矩阵空间，低分辨图像X，在h*w的矩阵空间。通常而言，认为X是由Y经过一些退化过程（统称为D）得到，D映射将图像从Y所在空间映射到X所在空间。该退化过程是未知的，模拟超分的流行方法就是认为该过程是Y与一个模糊核卷积之后降采样再加上一个高斯分布的噪声，得到X。从X得到Y的过程是退化过程的逆过程，在DL方法中，超分变成一个优化问题如下：<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-1.png" alt="alt text"><br>其中Isr是Ix经过网络生成的，L是损失函数，用于衡量Isr与Iy在Y空间中的距离，Φ是对模型的正则化约束，需要找到使右式最小时对应的网络参数θ。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><strong>退化模型</strong>，真实世界的配对图像难以获取，且包含更复杂的影响因素，所以常常通过HR进行模拟的退化过程得到配对的LR，退化方法包括BI，BD，DN。</p>
<h3 id="上采样方法"><a href="#上采样方法" class="headerlink" title="上采样方法"></a>上采样方法</h3><p>超分的目标是将X空间的图像映射到Y空间，尺寸扩大的过程被称作上采样，这是一种粗糙的超分方法，同时也是其他超分方法的预处理中重要的一步。<br>上采样方法包括，<br>插值：近邻插值，双线性插值，双三次插值<br>转置卷积：（1）添加边缘扩大尺寸，然后卷积。（2）插值添加间隙，然后卷积。<br>子像素卷积：先通过卷积增加通道数，然后h*w*C*r^2 -&gt; rh*rw*C 其中C是特征图通道数</p>
<h3 id="优化对象"><a href="#优化对象" class="headerlink" title="优化对象"></a>优化对象</h3><h4 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h4><p><strong>有监督</strong>：LR和HR配对，模拟超分中LR通过HR降采样产生，真实超分通过改变相机焦距获得，构建端到端的映射然后优化即可。<br><strong>无监督</strong>：使用未配对的LR和HR进行训练，包括弱监督自监督等，多数用GAN，如CinCGAN同时学习退化过程和超分过程的两个映射。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><strong>像素损失</strong>：顾名思义，就是比较两个图像的像素级别的差异，有对应位置差的绝对值L1loss或者对应位置差的平方mse等，此类损失函数得到的模型的SR往往缺少高频细节，视觉效果差<br><strong>内容损失</strong>（也叫感知损失）：使用预训练的分类网络（VGG，ResNet等）来评估图像的语义差异，取两个图像的高层图像特征作差求平均。<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-3.png" alt="alt text"><br><strong>对抗损失</strong>：GAN网络使用，G生成器需要最大化D认为G生成的Isr为真的概率，D需要认为Iy为真Isr为假<br><strong>先验损失</strong>：包括稀疏先验，边缘先验，梯度先验。可以使模型更快收敛并包含更多纹理细节，total variation：<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-2.png" alt="alt text"><br>其中E需要通过边缘探测器获得。<br><strong>傅里叶频率空间损失</strong><br>SR与频率高度相关，在下采样的过程中只有高频细节被抹除，所以损失函数用来比较输出图像和真实图像的频率空间的幅度和幅角。<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-4.png" alt="alt text"><br><strong>混合损失</strong><br>由于图像精度指标和感知精度指标是tradeoff的关系，需要通过调整超参数，平衡他们之间的关系。</p>
<h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p>有主客观的评价指标，客观的评价基于具体公式对图像进行计算，称为失真度量。主观的评价主要模拟人的视觉感知，使图像更加逼真。</p>
<h4 id="图像恢复精度"><a href="#图像恢复精度" class="headerlink" title="图像恢复精度"></a>图像恢复精度</h4><p><strong>PSNR</strong>：峰值信噪比，通过MSE定义，公式：<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-5.png" alt="alt text"><br>只关注像素级别的差异，从而难以捕捉整体的感知差异。<br><strong>SSIM</strong>：结构相似性指数，以一定的感知作为基础。包括结构亮度对比度，认为相邻的像素之间有强的相关性，公式：<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-6.png" alt="alt text"></p>
<h4 id="图像感知精度"><a href="#图像感知精度" class="headerlink" title="图像感知精度"></a>图像感知精度</h4><p><strong>平均评价分</strong>：请志愿者打分，主观性很强<br><strong>感知图像补丁相似度</strong>：LPIPS，使用预训练的模型，如VGG，将两个图像输入网络之后，在每个特征层都提取结构计算L2范数差异，公式：<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-7.png" alt="alt text"><br>Φl是预训练模型的第l个特征提取层，wl是权重参数<br><strong>深度图像结构与纹理相似性</strong>：DISTS利用由 CNN 构建的注入可微分函数将图像转换为多尺度超完全表示法，在这种表示法中，特征图的空间平均值可以捕捉纹理外观，并与人类对图像质量的评价相匹配。使用随机梯度下降算法时，比ssim需要更多的迭代次数从噪声中恢复，公式：<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-8.png" alt="alt text"><br><strong>自然图像质量评估器</strong>：NIQE，盲图像超分评估方法。只利用在自然图像中观察到的可测量的统计规律性偏差。根据自然场景统计（NSS）模型从图像中提取一组局部特征，然后将特征向量拟合到多元高斯（MVG）模型中。然后，通过 MVG 模型与从自然图像中学习到的 MVG 模型之间的距离来预测测试图像的质量。越小感知精度越好<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-9.png" alt="alt text"><br><strong>MA</strong>：基于学习的无参考图像质量评估。针对SR图像，通过局部&#x2F;全局频率变化和空间不连续性量化SR伪影，分别由三个独立的可学习回归森林建模，拟合感知分数，最后加权求和得到Ma。但是只能评估所训练的失真类型引起的质量下降。<br><strong>PI</strong>：MA和NIQE结合<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-10.png" alt="alt text"><br>越小越好。近年来用得多。</p>
<h2 id="模拟SISR"><a href="#模拟SISR" class="headerlink" title="模拟SISR"></a>模拟SISR</h2><h3 id="高效的网络-算法设计方法"><a href="#高效的网络-算法设计方法" class="headerlink" title="高效的网络&#x2F;算法设计方法"></a>高效的网络&#x2F;算法设计方法</h3><p><strong>残差学习</strong>，消除梯度消失和退化的问题，全局和局部残差学习，跳跃连接，bn层不提升效果，一般移除。ResNet，VDSR。<br><strong>Dense</strong>，每个卷积块接受前面所有卷积块输出的叠加作为输入。如SRDenseNet RDN<br><strong>递归学习</strong>，反复使用共享参数的递归块，还可以和残差学习结合起来，如MemNet CARN SRRFN.<br><strong>反馈机制</strong>，不同于递归学习，参数不共享，只是输出会被带入输入，细化底层细节，引导LR图像恢复出高质量SR。 DBPN DSRN SFRBN<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-11.png" alt="alt text"></p>
<p><strong>多尺度学习</strong>，MSRB集成各种卷积核，自适应不同尺度的图像特征。MSFFRN多尺度特征融合残差。MSDN多尺度+密集连接。MSRCAN多尺度+注意力机制。<br><strong>注意力机制</strong>，通道注意：引入SE机制squeeze and excitation，如在前面加上全局平均池化和一个sigmoid函数对每个通道进行重定标，使网络集中在更有用的通道。还有一些网络例如SAN等基于其他统计量进行归一化。<br>还有使用非局部注意作为过滤算法，计算所有像素的加权平均，这样远处的像素也能影响相关位置，RNAN HAN CSNLN ENLCA<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-12.png" alt="alt text"></p>
<p><strong>门控机制</strong>，跳跃链接使得通道维度太高了，认为跳转连接后的输出特征应该被有效地重新融合，而不是简单地连接。MemNet CARN 可以和注意力机制结合。<br><strong>基于转换器的方法</strong>，源自自然语言处理，先用预训练的模型（称为transformer）捕捉图像全局信息，提高重建图像的质量，参数数量比较大。transformer具体是啥还没看，和NLP相关。 SwinIR CAT HAT GRL ESRT ELAN<br>优点是性能很高，缺点是占用大量gpu</p>
<h3 id="感知精度方法"><a href="#感知精度方法" class="headerlink" title="感知精度方法"></a>感知精度方法</h3><p>致力于提升感知精度，以下是主要的修改方向：<br><strong>使用感知损失函数</strong>：除前面提到的内容损失，还有纹理损失，目标感知损失。纹理损失即风格重建损失，定义为SR与ground truth的格兰矩阵之间差值的平方。<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-13.png" alt="alt text"><br><strong>对抗训练</strong>：SRGAN<br><img src="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/image-14.png" alt="alt text"><br>为了避免GAN生成无意义的高频细节，SRFeat使用两个判别器，一个常规，一个从VGG提取的高层图像特征用于判断。ESRGAN使用密集残差替换原始残差，用相对平均真实性判别器。<br>循环一致性，G和F两个生成器模拟LR到HR和HR到LR，GF的叠加应该是恒等映射。<br>CinCGAN将有噪声的模糊输入映射到无噪声的 LR 域，然后使用预先训练好的模型进行上采样。DRN通过学习从 HR 到 LR 图像的映射来估计下采样核并重建 LR 图像，从而形成一个闭环，提供额外的监督。<br><strong>扩散方法</strong>，基于DDPM，通过马尔可夫链将高斯噪声逐渐转换为以 LR 为输入条件的 SISR 图像，从而提供了多样化和真实的 SISR 预测。比GAN更逼真，需要大量样本，收敛很慢。 SRdiff SR3 IDM DR2 DiffIR</p>
<h3 id="额外辅助信息优化方法"><a href="#额外辅助信息优化方法" class="headerlink" title="额外辅助信息优化方法"></a>额外辅助信息优化方法</h3><p><strong>内部统计</strong>，利用图像内部统计数据来进一步提高模型性能。在 ZSSR中，内部图像统计特性被用于训练特定图像的 CNN，其中训练示例是从测试图像本身中提取的。在训练阶段，通过数据增强生成多个 LR-HR 对，并用这些对训练 CNN。测试时，将 LR 图像𝐼𝐿𝑅 作为输入输入到训练好的 CNN，以获得重建图像。感觉直接拿测试数据进行预训练有点。。。SinGAN 金字塔型的全卷积GAN，学习图像中不同尺度的patch分布，利用内部信息的递归性，进行了多次上采样。<br><strong>多因素学习</strong>，不限制于固定的上采样因子。逐步重构超分，分辨率不断提升，可以在中途获取中间结果作为相应的因子比较低的结果。MDCN 在MDSR基础上在模型头尾添加尺度处理模块，学习尺度间的相关性。<br><strong>先验指导</strong>，如TV，phrase，edge先验，可以与各种训练方法和网络结构结合。 DEGREE结合递归和边缘先验。SeaNet学习图像边缘而非应用现成的边缘检测器。GAN可以和很多先验结合，如SFTGAN SPSR FeMaSR等。<br><strong>基于参考图像的方法</strong>，将参考图像和LR对齐之后。CrossNet 提出光流方法来对齐不同尺度的参考图像和 LR 图像，然后将其串联到解码器的相应层中。还有人利用 LR 图像和参考图像的 VGG 特征之间的补丁匹配，将参考图像中的纹理自适应地转移到 LR 图像中。在 TTSR 提出了一种纹理转换器网络，用于从参考图像中搜索相关纹理并将其转换到 LR 图像中。</p>
<p><strong>知识蒸馏</strong>，翻译是这样的，大概就是将大模型的中间特征层转移到子模型上继续训练，适应一个更小范围的功能。如JDSR CSD等。</p>
<h2 id="真实世界SISR"><a href="#真实世界SISR" class="headerlink" title="真实世界SISR"></a>真实世界SISR</h2><h3 id="盲图像SISR"><a href="#盲图像SISR" class="headerlink" title="盲图像SISR"></a>盲图像SISR</h3><p>退化模式未知的情况下进行超分</p>
<h4 id="显式退化建模"><a href="#显式退化建模" class="headerlink" title="显式退化建模"></a>显式退化建模</h4><p>基于经典退化模型，模糊核和加性噪声，一种用于多重退化学习的简单、可扩展的深度 CNN 框架（SRMD）。在 SRMD 中，串联的 LR 图像和降解图作为网络的输入，以实现不同降解条件下的图像超分辨率。在 SRMD 的基础上，UDVD利用动态卷积来处理图像中不同区域的不同降解。这类方法通常依赖可靠的退化估计方法来快速获得令人满意的 SR 输出。因此，将退化估计纳入 SR 框架的方法将获得更稳定、更可靠的结果。这类方法将退化估计和 SR 过程结合到一个统一的模型中，其中核估计是主要的研究工作。例如，IKC 提出了迭代核校正程序，以帮助盲 SISR 任务找到更准确的模糊核。深度交替网络DAN，将模糊核估计和 SR 图像还原在一个网络中，使还原器和估计器很好地兼容，从而在核估计方面取得了很好的效果。虽然这种方法比使用现成的估计算法更稳健，但这种迭代方案往往会消耗更多的推理时间，并可能因估计误差过大而导致 SR 失效。为了解决这个问题，一些研究引入了更精确的退化估计方法。DASR退化感知网络，该网络可根据学习到的表征灵活适应各种退化。还有一些方法可以从真实图像中估算出更真实的内核。</p>
<h4 id="隐式退化建模"><a href="#隐式退化建模" class="headerlink" title="隐式退化建模"></a>隐式退化建模</h4><p>对退化过程进行隐式建模的盲 SISR 方法旨在通过学习外部数据集来建立退化模型。这类方法通常通过 GAN 框架学习数据分布，并使用一个或多个判别器来区分生成的图像和真实图像。CinCGAN 一种无监督图像 SR ，首先将有噪声的模糊输入映射到无噪声的低分辨率空间，然后用预先训练好的模型对中间图像进行上采样。最后，以端到端的方式对这两个模块进行微调，以获得 SR 输出。Bulat认为现实世界中的低分辨率图像构成了高维空间中的特定分布，并使用生成式对抗网络从高分辨率图像中生成符合这种分布的低分辨率图像。之后，Yuan  和 Maeda 等人 进一步提出了一个统一的框架，可以同时学习伪低分辨率图像的生成和高分辨率图像的重建，在实际场景中取得了较好的效果。Wei 等人进一步考虑了伪低分辨率图像与真实低分辨率图像之间的领域差异，提出了一种领域适应机制来提高模型性能。DeFlow 框架，利用流模型的随机建模能力来增强伪低分辨图像的多样性，进一步提高了真实场景中的图像超分辨性能。</p>
<h3 id="任意尺度超分"><a href="#任意尺度超分" class="headerlink" title="任意尺度超分"></a>任意尺度超分</h3><p>单一模型处理任意比例因子， Meta-SR 和 Meta-USR 可用于任意退化模式，包括非整数比例因子。引入了一个插件模块，使用条件卷积来根据输入的尺度信息动态生成滤波器，因此配备了所提出的模块的网络只需一个模型就能实现任意尺度的良好效果。</p>
<h1 id="个人觉得比较好的method"><a href="#个人觉得比较好的method" class="headerlink" title="个人觉得比较好的method"></a>个人觉得比较好的method</h1><p>兼顾实用性和对医学图像的可行性<br>我们的退化过程是PET仿真的，因此自然图像的模拟退化过程并不需要。上采样方法可以考虑使用。做仿真时可以考虑训练有监督和无监督，看看效果差异，可以考虑有监督作为预训练或者分类网络，后面用无监督。损失函数需要包含合适的先验损失，特别是医学图像特有的先验。评估方法主要还是图像恢复精度，感知精度确保纹理相似性就行。网络设计选择很多，需要先排除一部分，值得讨论。额外信息优化方法需要考虑一下。盲图像超分中显式退化建模的方法不知道能不能修改成建模PSF和LSF的模型，对这俩进行估计。隐式退化建模中的几个网络都是无监督的，比较适合直接使用</p>
<h1 id="发展趋势"><a href="#发展趋势" class="headerlink" title="发展趋势"></a>发展趋势</h1><p>论文主要来自ECCV ICCV CVPR ICCS CVPRW ICCVW ACMMM等</p>
<h2 id="2014以前"><a href="#2014以前" class="headerlink" title="2014以前"></a>2014以前</h2><p>提出ssim 2001，讨论评估方法 Image quality assessment: from error visibility to structural similarity.<br>邻域嵌入neighbor embedding，2004. Super-resolution through neighbor embedding. 2012. Low-complexity singleimage super-resolution based on nonnegative neighbor embedding.<br>轮廓先验（算图像梯度图），轮廓检测和分层图像分割，Image super-resolution using gradient profile prior. 2010. Contour detection and hierarchical image segmentation.<br>稀疏回归和自然图像先验，稀疏表示: 2010. Single-Image Super-Resolution Using Sparse Regression and Natural Image Prior. 2010. Image super-resolution via sparse representation. 2010. One single image scale-up using sparse-representations. 2011. Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization. </p>
<h2 id="2014"><a href="#2014" class="headerlink" title="2014"></a>2014</h2><p>SRCNN：2014. Learning a deep convolutional network for image super-resolution.<br>GAN：2014. Generative adversarial nets.<br>VGG： 2014. Very deep convolutional networks for large-scale image recognition.</p>
<h2 id="2015"><a href="#2015" class="headerlink" title="2015"></a>2015</h2><p>反馈机制： 2015. Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks.<br>纹理损失： 2015. Texture synthesis using convolutional neural networks. 2015. A neural algorithm of artistic style.<br>知识蒸馏： 2015. Distilling the knowledge in a neural network.<br>为SRGAN的判别器 2015. Unsupervised representation learning with deep convolutional generative adversarial networks.</p>
<h2 id="2016"><a href="#2016" class="headerlink" title="2016"></a>2016</h2><p>ResNet残差学习残差块：Deep residual learning for image recognition.<br>迭代反馈：Human pose estimation with iterative error feedback.<br>FSRCNN使用转置卷积：Accelerating the super-resolution convolutional neural network.<br>感知损失引入：Perceptual losses for real-time style transfer and super-resolution.<br>在残差学习的基础上VDSR出现：Accurate image super-resolution using very deep convolutional networks.<br>递归学习共享参数的DRCN：Deeply-recursive convolutional network for image superresolution.<br>子像素卷积层作为上采样方法的ESPCN：Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network.<br>面部识别超分的应用：Deep cascaded bi-network for face hallucination.</p>
<h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><p>多尺度残差块学习MSRB：Deep learning with depthwise separable convolutions.<br>FID评价指标，比较SR和真值的数据分布：Gans trained by a two time-scale update rule converge to a local nash equilibrium.<br>密集链接DenseNet：Densely connected convolutional networks.<br>progress learning、多尺度学习：LapSRN：Deep laplacian pyramid networks for fast and accurate super-resolution.<br>SRGAN，使用了GAN，全局局部残差连接，内容损失，G是ResNet：Photo-realistic single image super-resolution using a generative adversarial network.<br>EDSR指出bn层没有提升训练效果，大尺度的网络可以使用预训练的小尺度网络参数初始化：Enhanced deep residual networks for single image super-resolution.<br>提出MA评估方法：Learning a no-reference quality metric for single-image super-resolution.<br>超分应用，高光谱超分：Hyperspectral image spatial superresolution via 3D full convolutional neural network.<br>残差在连接到全局时乘以一个常数：Inception-v4, inception-resnet and the impact of residual connections on learning.<br>递归学习DRRN：Image super-resolution via deep recursive residual network.<br>MemNet递归学习密集连接门控机制：A persistent memory network for image restoration.<br>SRDenseNet密集连接块的创新，通道连接门控机制：Image super-resolution using dense skip connections.<br>NLP领域Transformer自注意力：Attention is all you need.<br>边缘先验和递归网络结合：Deep edge guided recurrent residual learning for image super-resolution.<br>提出SycleGAN：Unpaired image-to-image translation using cycle-consistent adversarial networks.</p>
<h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><p>轻量化级联残差网络：Fast, accurate, and lightweight super-resolution with cascading residual network.<br>指出克服失真和追求感知精度是矛盾的：The perception-distortion tradeoff.<br>现实世界中的低分辨构成高维空间的特定分布：To learn image super-resolution, use a GAN to learn how to do image degradation first.<br>医学图像：Efficient and accurate MRI super-resolution using a generative adversarial network and 3D multi-level densely connected network.</p>
<p>NLP领域的：Pre-training of deep bidirectional transformers for language understanding.</p>
<p>知识蒸馏：Image super-resolution using knowledge distillation.</p>
<p>DSRN反馈机制：Image super-resolution via dual-state recurrent networks.</p>
<p>提出SE块，引导模型注意力：Squeeze-and-excitation networks.</p>
<p>知识蒸馏IDN：Fast and accurate single image super-resolution via information distillation network.</p>
<p>提出了一种新的D：The relativistic discriminator: a key element missing from standard GAN.</p>
<p>MSRB多尺度：Multi-scale Residual Network for Image SuperResolution.</p>
<p>非局部注意NLRN：Non-local recurrent network for image restoration.</p>
<p>假如SE机制的通道注意：An Effective Single-Image Super-Resolution Model Using Squeeze-and-Excitation Networks.</p>
<p>SRFeat采用了两个判别器，图像和特征：Srfeat: Single image super-resolution with feature discrimination.</p>
<p>ZSSR无监督，数据增强：?zero-shot? super-resolution using deep internal learning.</p>
<p>非局部：Non-local neural networks.</p>
<p>空间变化SFT，纹理更逼真自然SFTGAN：Recovering realistic texture in image super-resolution by deep spatial feature transform.</p>
<p>Esrgan纯残差密集连接块去除bn层: Enhanced super-resolution generative adversarial networks.</p>
<p>CinCGAN：Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks.</p>
<p>SRMD多种退化模式：Learning a single convolutional super-resolution network for multiple degradations.</p>
<p>提出LPIPS评价方法：Learning a single convolutional super-resolution network for multiple degradations.</p>
<p>通道注意RCAN：Image Super-Resolution Using Very Deep Residual Channel Attention Networks.</p>
<p>光流方法对齐辅助信息Crossnet: An end-to-end reference-based super resolution network using cross-scale warping.</p>
<h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><p>有很多应用论文了，包括遥感，高光谱，人脸等<br>MSRCAN多尺度密集连接注意力：Single image super-resolution via multi-scale residual channel attention network.</p>
<p>SAN：Second-order attention network for single image super-resolution.</p>
<p>Blind super-resolution with iterative kernel correction.</p>
<p>Meta-SR: A magnificationarbitrary network for super-resolution.</p>
<p>轻量知识蒸馏：Lightweight image super-resolution with information multi-distillation network.</p>
<p>SRRFN：Lightweight and Accurate Recursive Fractal Network for Image Super-Resolution.</p>
<p>SRFBN：Feedback network for image super-resolution.</p>
<p>感知精度损失的创新：Targeted perceptual loss for single image super-resolution.</p>
<p>GPT：Language models are unsupervised multitask learners.</p>
<p>Singan不同尺度内部信息学习: Learning a generative model from a single natural image.</p>
<p>AWSRN-S：Lightweight image super-resolution with adaptive weighted learning network.</p>
<p>Ranksrgan: Generative adversarial networks with ranker for image super-resolution.</p>
<p>RNAN非局部注意块，通道和空间注意：Residual non-local attention networks for image restoration.</p>
<p>利用LR和参考图像中的vgg特征之间的补丁匹配，将参考图像中的纹理自适应的转移到LR中：Image super-resolution by neural texture transfer.</p>
<p>CSN：Channel splitting network for single MR image super-resolution.</p>
<h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><p>Image quality assessment: Unifying structure and texture similarity.</p>
<p>SeaNet边缘先验检测器：Soft-edge assisted network for single image super-resolution.</p>
<p>DRN同时学习下采样过程：Closed-loop matters: Dual regression networks for single image super-resolution.</p>
<p>DBPN：Deep back-projectinetworks for single image super-resolution.</p>
<p>扩散模型：Denoising diffusion probabilistic models.</p>
<p>Madnet: A fast and lightweight network for single-image super resolution.</p>
<p>MDCN，MDSR的升级版: Multi-scale dense cross network for image super-resolution.</p>
<p>RFANet：Residual feature aggregation network for image super-resolution.</p>
<p>DAN交替优化算法找模糊核：Unfolding the alternating optimization for blind super resolution.</p>
<p>梯度图指导图像复原：Structure-preserving super resolution with gradient guidance.</p>
<p>无监督盲图像超分：Unpaired image super-resolution using pseudo-supervision.</p>
<p>CSNLN：同一特征图中LRHR的长距离以来关系：Image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining.</p>
<p>Single image super-resolution via a holistic attention network. </p>
<p>多尺度融合残差：Multi-scale feature fusion residual network for Single Image Super-Resolution.</p>
<p>超分和语义分割整合在一起：Dual super-resolution learning for semantic segmentation.</p>
<h2 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h2><p>IPT基于Transformer：Pre-trained image processing transformer.</p>
<p>傅里叶空间损失：Fourier space losses for efficient perceptual image super-resolution.</p>
<p>解决了参数数量庞大的问题SwinIR: Image restoration using swin transformer.</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows.</p>
<p>ESRT：Transformer for Single Image Super-Resolution.</p>
<p>SMSR：Exploring sparsity in image super-resolution for efficient inference.</p>
<p>学习抽象表征来区分表征空间中的各种退化，引入退化感知网络：Unsupervised Degradation Representation Learning for Blind Super-Resolution.</p>
<p>空间距离感知：Unsupervised real-world image super resolution via domain-distance aware training.</p>
<p>流模型随机建模Deflow: Learning complex image degradations from unpaired data with conditional flows.</p>
<h2 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h2><p>FeMaSR预训练得到的离散特征作为先验：Real-world blind super-resolution via feature matching with implicit high-resolution priors.</p>
<p>拓展注意区域CAT：Cross Aggregation Transformer for Image Restoration.</p>
<p>结合Diffusion model Srdiff: Single image super-resolution with diffusion probabilistic models.</p>
<p>结合stable diffusion：High-resolution image synthesis with latent diffusion models.</p>
<p>降低HR特征图的计算复杂度Uformer: A general u-shaped transformer for image restoration.</p>
<p>Restormer: Efficient transformer for high-resolution image restoration.</p>
<p>ELAN共享自注意力机制，降低模型复杂度，加速基于transformer的模型：Efficient long-range attention network for image superresolution.</p>
<p>Efficient image super-resolution using vast-receptive-field attention.</p>
<h2 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h2><p>DAT：Dual aggregation transformer for image super-resolution.</p>
<p>IDM隐式神经表征学习连续分辨率表征：Implicit Diffusion Models for Continuous Super-Resolution.</p>
<p>Diffbir: Towards blind image restoration with generative diffusion prior.</p>
<p>HPUN-L：Hybrid pixel-unshuffled network for lightweight image super-resolution.</p>
<p>利用StableSR预训练的文本到图像模型：Exploiting Diffusion Prior for Real-World Image Super-Resolution.</p>
<p>在ground truth上预训练的Diffir: Efficient diffusion model for image restoration.</p>
<p>ARFFT加强表征能力拓展感受野：Attention Retractable Frequency Fusion Transformer for Image Super Resolution.</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/07/08/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/06/27/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E7%9B%B8%E5%85%B3/">
        <h2 class="post-title">服务器使用相关</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                tools
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/6/27
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>科研过程中如果需要跑深度学习的训练，常常可能会用到实验室的服务器计算资源。这里记录一下我的服务器使用和配置相关，以后用其他地方的服务器按流程操作即可。</p>
            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/06/27/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E7%9B%B8%E5%85%B3/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/06/27/git%E4%BD%BF%E7%94%A8/">
        <h2 class="post-title">git使用</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                tools
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/6/27
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p><a target="_blank" rel="noopener" href="https://git-scm.com/book/zh/v2">指路官方文档</a>，感觉已经写得很全面了。主要还是要有用的需求，然后平时才会主动去用…</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/06/27/git%E4%BD%BF%E7%94%A8/" class="go-post">阅读全文</a>
</div>


        <div class="page-current">
    
    <span class="current">1</span>
    
    <a class="page-num" href="/page/2">2</a>
    
    
    
    
    <a class="page-num" href="/page/2/">
        <i class="fa-solid fa-caret-right fa-fw"></i>
    </a>
    
</div>

    </div>
    
    <div id="home-card">
        <div id="card-style">
    <div id="card-div">
        <div class="avatar">
            <img src="/images/avatar1.jpg" alt="avatar" />
        </div>
        <div class="name">Knight Cauchy</div>
        <div class="description">
            <p>good good study, day day up</p>

        </div>
        
        <div class="icon-links">
            
            <span class="icon-link">
                <a href="/function%20link()%20%7B%20%5Bnative%20code%5D%20%7D">
                    <i
                        class="fa- fa- fa-fw"
                    ></i>
                </a>
            </span>
            
        </div>
        
        
    </div>
</div>

    </div>
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 橙橙子の小站
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Knight Cauchy
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <!-- 添加 <canvas> 元素 -->
        <canvas
        id="fireworks"
        style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: 32767"
    ></canvas>
    <!-- 添加自定义JavaScript文件 -->
    <!-- <script src="/js/your_custom_file.js"></script> -->
    <script src="/js/main.js"></script>
    <!-- 添加 fireworks.min.js -->
    <script src="/js/fireworks.min.js"></script>
    
</body>
</html>
